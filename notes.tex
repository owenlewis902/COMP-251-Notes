\documentclass{article}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}

\graphicspath{{pics/}}

%CURLIES  :)       $\{$ $\}$

\newcommand{\ti}[1]{\textit{#1}}
\newcommand{\tb}[1]{\textbf{#1}}
\newcommand{\rn}[1]{\romannum{#1}}
\newcommand{\Rn}[1]{\Romannum{#1}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\Om}{\Omega}
\newcommand{\om}{\omega}
\newcommand{\la}{\lambda}
\newcommand{\ep}{\varepsilon}
\newcommand{\emp}{\emptyset}
\newcommand{\lt}{\textless}
\newcommand{\gt}{\textgreater}
\newcommand{\imply}{\Rightarrow}
\newcommand{\x}{\cdot}
\newcommand{\Ga}{\Gamma}
\newcommand{\al}{\alpha}
\newcommand{\sg}{\sigma}
\newcommand{\be}{\beta}
\newcommand{\prop}{\textbf{Proposition: }}
\newcommand{\thm}{\textbf{Theorem: }}
\newcommand{\lem}{\textbf{Lemma: }}
\newcommand{\cor}{\textbf{Corollary: }}
\newcommand{\proo}{\textbf{Proof: }}
\newcommand{\exx}{\textbf{Example: }}
\newcommand{\exxi}{\textbf{Example 1: }}
\newcommand{\exxii}{\textbf{Example 2:  }}
\newcommand{\exxiii}{\textbf{Example 3:  }}
\newcommand{\soln}{\textbf{Solution: }}
\newcommand{\ds}{\displaystyle}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\ceil}[1]{\lceil #1 \rceil}
\newcommand{\bigfloor}[1]{\big\lfloor #1 \big\rfloor}
\newcommand{\bigceil}[1]{\big\lceil #1 \big\rceil}
\newcommand{\Bigfloor}[1]{\Big\lfloor #1 \Big\rfloor}
\newcommand{\Bigceil}[1]{\Big\lceil #1 \Big\rceil}
\newcommand{\biggfloor}[1]{\bigg\lfloor #1 \bigg\rfloor}
\newcommand{\biggceil}[1]{\bigg\lceil #1 \bigg\rceil}
\newcommand{\Biggfloor}[1]{\Bigg\lfloor #1 \Bigg\rfloor}
\newcommand{\Biggceil}[1]{\Bigg\lceil #1 \Bigg\rceil}

\title{COMP 251: Algorithms \& Data Structures}
\author{Owen Lewis}
\date{Winter 2018}

\begin{document}

\begin{titlepage}
\maketitle
\end{titlepage}

\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Overview of Graph Theory}
\subsection{Definitions}
A graph $G = (V, E)$ is a set $V$ of vertices (a.k.a. nodes) and a set $E$ of edges (denoting vertex pairs). We set $n = |V|$, and $m = |E|$. A graph is said to be \ti{undirected} when for any edge $(u, v) \in E$ there exists an edge $(v, u) \in E$ for some nodes $u$, and $v$. A graph is said to be $\ti{directed}$ if it is not undirected. In other words, the edge set of a directed graph consists of ordered pairs where the edge set of an undirected graph consists of unordered pairs.\\\\
A \ti{walk} is a set of vertices $\{v_0, v_1, \dots, v_{\ell}\}$ such that $(v_i, v_{i+1}) \in E$, $\forall\ 0 \leq i \leq \ell$. 
A walk where $v_0 = v_{\ell}$ is said to be a \ti{circuit} or a \ti{closed walk}.
A circuit where every edge in the graph is used exactly once is known as an \ti{Eulerian circuit}. 
A \ti{cycle} is a walk $\{v_0, v_1, \dots, v_{\ell}\}$ such that every vertex is distinct except $v_0 = v_{\ell}$.
A cycle where every vertex of the graph is used exactly once is known as a \ti{Hamiltonian cycle}.
A walk where every vertex is distinct is said to be a \ti{path}.\\\\
A graph is said to be \ti{connected} if for each $u, v \in V$ there exists a walk from $u$ to $v$.
A graph is said to be \ti{disconnected} id it is not connected.
Each connected subgraph of a graph is called a \ti{component}. A connected graph therefore has exactly one component.\\\\
A connected component with no cycles is called a \ti{tree}. A graph whose components are all trees is said to be a \ti{forrest}. A tree is said to be \ti{spanning} if it contains every vertex in the graph. A vertex in a tree with at most one neighbour is called a \ti{leaf}.\\\\
A \ti{matching} is a set of vertex-disjoint edges i.e. each edge is incident to at most one other edge in a matching. A matching is said to be \ti{perfect} if every vertex is incident to exactly one edge in the matching.\\\\
A \ti{clique} is a set of pairwise adjacent vertices. In \ti{independent set} (a.k.a. a \ti{stable set}) is set of pairwise non-adjacent vertices.\\\\
A \ti{bipartite graph} is a graph such that the vertex set $V$ can be partitioned as $V = X \cup Y$ where each edge has one node in $X$ and the other node in $Y$. Note that $X$ and $Y$ are necessarily independent sets.
\subsection{Some Theorems for Undirected Graphs}
\thm (Handshaking Lemma) Let $G = (V, E)$ be an undirected graph, let $\Ga(v) := \{u : (u, v) \in E\}$ be the set of neighbours of a node $v$, and let the \ti{degree} $\deg(v)$ of a vertex $v$ equal the cardinality of $\Ga(v)$. Then there are an even number of vertices with odd degree.\\
\proo First note that since we're double-counting the number of pairs where $(v, e)$ is an edge incident to $v$
\[2 \x |E| = \sum_{v \in V} \deg(v) \]
Since the degree of a vertex is either even or odd, we can partition $V$ into a set of odd-degree vertices $\mathcal{O}$, and a set of even-degree vertices $\mathcal{E}$. This gives us
\[\sum_{v \in V} \deg(v) = \sum_{v \in \mathcal{O}} \deg(v) + \sum_{v \in \mathcal{E}} \deg(v)\]
which implies
\[\sum_{v \in \mathcal{O}} \deg(v) = 2\x |E| - \sum_{v \in \mathcal{E}} \deg(v)\]
since both the $2\x |E|$ term is even (obvious) and the $\displaystyle \sum_{v \in \mathcal{E}} \deg(v)$ term is even (sum of even numbers) then the $\displaystyle \sum_{v \in \mathcal{O}} \deg(v)$ term must also be even.
\qed\\\\
\thm (Euler's Theorem) If $G$ is an undirected graph then $G$ contains an Eulerian circuit if and only if every vertex has even degree.\\
\proo Easy proof by induction\\\\
\lem A tree $T$ with $n \geq 2$ vertices has at least one leaf vertex.\\
\proo Trees are connected so there exists no vertices with degree 0 when $n \geq 2$. Suppose each vertex has degree of at least 2. Then consider the longest path $P \subseteq T$, $P = \{v_1, v_2, \dots, v_{\ell-1}, v_{\ell}\}$. Since $\deg(v_{\ell}) \geq 2$, $\exists$ a neighbour (of $v_{\ell}$) $x \in P$ with $x \neq v_{\ell-1}$. If $x = v_{\ell + 1}$ then $P$ is not the longest path, a contradiction. Therefore, for $P$ to be the longest path, $x$ must be somewhere else in $P$, but this creates a cycle, another contradiction. Thus there must exist at least one node $v$ such that $0\ \lt\ \deg(v)\ \lt\ 2$ -- a leaf.
\qed\\\\
\thm A tree with $n$ vertices has exactly $n-1$ edges.\\
\proo Simple proof by induction.\\
\ti{Base case:} A tree with one vertex trivially has 0 edges.\\
\ti{Induction Hypothesis:} Assume any tree with $n-1$ vertices has $n-2$ edges.\\
\ti{Inductive Step:} Take a tree with $n \geq 2$ vertices. By the previous lemma this tree contains a leaf vertex $v$. This implies that $T\setminus\{v\}$ is a tree with $n-1$ vertices and by the induction hypothesis $T \setminus \{v\}$ is a tree with $n-2$ edges, which imples that $T$ is a tree with $n-1$ edges.
\qed\\\\
\thm (Hall's Theorem) Let $G = (X \cup Y, E)$ with $|X| = |Y|$ be a bipartite graph. $G$ contains a perfect matching if and only if $\forall\ B \subseteq X$, $|\Ga(B)| \geq |B|$ (Hall's condition).\\
\proo Firstly, the ($\imply$) direction is fairly obvious. If $B \subseteq X$ with $\Ga(B)\ \lt\ |B|$ then the graph can't have a perfect matching. The ($\Leftarrow$) direction is a bit trickier. Suppose Hall's condition is satisfied. Then, take the maximum cardinality matching $M$ is the graph. If $M$ is perfect then we are done. Otherwise there must exist an unmached vertex $b_0$.
\begin{itemize}
	\item Since Hall's condition holds, we have $|\Ga(\{b_0\})| \geq |\{b_{0}\}| = 1$ so $b_0$ must have at least one neighbour $s_0$.
	\item Suppose $s_0$ is matched in $M$ to $b_1$.
	\item Since Hall's condition holds, we have $|\Ga(\{b_0, b_1\})| \geq |\{b_{0}, b_1\}| = 2$ so $\{b_0, b_1\}$ must have at least one neighbour $s_1 \neq s_0$.
	\item Suppose $s_1$ is matched in $M$ to $b_2$.
	\item Since Hall's condition holds, we have $|\Ga(\{b_0, b_1, b_2\})| \geq |\{b_{0}, b_1, b_2\}| = 3$ so $\{b_0, b_1, b_2\}$ must have at least one neighbour $s_2 \notin \{s_0, s_1\}$.
	\item \dots
\end{itemize}
we repeat this argument as long as we can. Since the graph contains a finite number of vertices this process must terminate, but it can only terminate when we reach an unmatched node $s_k$. Using the edges we've formed in $M$ we can create a path $P$ from $b_0$ to $s_k$ that alternates between using non-matching edges and using matching edges. Swapping the matching edges with the non-matching edges gives us one more matching edge (as we have an odd number of edges.) This is still a valid matching as the internal nodes of $P$ are still incident to exactly one matching edge. Also, the end nodes, $b_0$ and $s_k$ were previously unmatched but are now incident to exactly one edge in the new matching. Thus $M$ isn't the maximum capacity matching -- a contradiction.
\qed
\subsection{``````Data Structures'''''' for Representing Graphs}
\subsubsection{Adjacency Matrices}
For a graph, an \ti{adjacency matrix} $M$ is a matrix suxh that
\begin{enumerate}
	\item There is a row for each vertex
	\item There is a column for each vertex
	\item The $ij-th$ entry is defined as $M_{ij} = 
\begin{cases}
1, (i, j) \in E\\
0, (i, j) \notin E
\end{cases}$
\end{enumerate}
Note that in an undirected graph the matric is symmetric around the diagonal because $(i, j) \equiv (j, i)$. Of course this is not necessarily true of directed graphs.
\subsubsection{Adjacency Lists}
An \ti{adjacency list} of an undirected graph is such that for each vertex $v$ of $V$ we store a list of its neighbours. For a directed graph we have two lists: one in which we store the in-neighbours of $v$ and one in which we store the out-neighbours of $v$.
\subsubsection{Adjacency Matrices vs. Adjacency Lists}
The main difference between the two is the amount of storage required to implement them.
\begin{itemize}
	\item An adjacency matrix requires we store $\Theta(n^2)$ numbers
	\item An adjacency list requires we store $\Theta(m)$ numbers
\end{itemize}
In any graph $m = O(n^2)$. This means that for a sparse graph adjacency lists are highly favourable in terms of space complexity.\\\\
Verifying whether an edge exists, however, is much faster in an adjacency matrix -- when using the array representation of a matrix it takes $O(1)$ time, where verifying the existance of an edge takes $O(\log n)$ time for an ordered adjacency list (using binary search), and $O(n)$ time if the adjacency list is not ordered (using sequential search).
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Divide \& Conquer}
A \ti{divide and conquer} algorithm ideally breaks up a problem of size $n$ into smaller sub-problems such that:
\begin{itemize}
	\item There are exactly $a$ sub-problems
	\item Each sub-problem has a size of at most $\displaystyle \frac{1}{b}\x n$
	\item Once solved, the solutions to the sub-problems must be combined in $O(n^d)$ time to produce a solution to the original problem
\end{itemize}
Therefore the time-complexity of a divide and conquer algorithm satisfies a recurrence relation given by
\[T(n) = a\x T\Big(\frac{n}{b}\Big) + O(n^d)\]
\subsection{The Master Theorem}
\tb{Lemma 1:} $\displaystyle \sum_{k=0}^{\ell} \tau^k = \frac{1-\tau^{\ell + 1}}{1-\tau}$, for any $\tau \neq 1$.\\
\proo
\begin{align*}
	(1-\tau)\sum_{k=0}^{\ell} \tau^k &= \sum_{k=0}^{\ell} \tau^k - \sum_{k=1}^{\ell+1} \tau^k\\
		&= \tau^0 - \tau^{\ell + 1}\\
		&= 1 - \tau^{\ell + 1}\\
	\sum_{k=0}^{\ell} \tau^k &= \frac{1 - \tau^{\ell + 1}}{1- \tau}
\end{align*}
\qed\\\\
\tb{Lemma 2:} $\displaystyle x^{\log_{b}y} = y^{\log_{b}x}$ for any base $b \in \R$.\\
\proo From the power rule of logarithms we have
\[\log_{b}x \x \log_{b}y = \log_{b}\big(y^{\log_{b}x}\big)\]
similarily, we have
\[\log_{b}x \x \log_{b}y = \log_{b}\big(x^{\log_{b}y}\big)\]
therefore
\[\log_{b}\big(x^{\log_{b}y}\big) = \log_{b}\big(y^{\log_{b}x}\big)\]
thus
\[x^{\log_{b}y} = y^{\log_{b}x}\]
\qed\\\\
\thm (The Master Theorem) If a recurrence relation is of the form $\displaystyle T(n) = a\x T\Big(\frac{n}{b}\Big) + O(n^d)$, for constants $a\ \gt\ 0$, $b\ \gt\ 1$, and $d \geq 0$, then
\[
T(n) =
\begin{cases}
	O(n^d) &\text{, if } a\ \lt\ b^d \text{ [Case I]}\\
	O(n^d\x \log n) &\text{, if } a = b^d \text{ [Case II]}\\
	O(n^{log_{b}a}) &\text{, if } a\ \gt\ b^d \text{ [Case III]}
\end{cases}
\]
\proo By adding dummy numbers, we may assume that $n$ is a power of $b$, i.e. $n = b^{\ell}$, for some $\ell \in \N_{0}$. Then
\[T(n) = n^d + a\Big(\frac{n}{b}\Big)^d + a^{2}\Big(\frac{n}{b^2}\Big)^d + \dots + a^{\ell}\Big(\frac{n}{b^{\ell}}\Big)^d\]
simlifying, we get
\[T(n) = n^d \bigg(1 + \frac{a}{b^d} + \Big(\frac{a}{b^d}\Big)^2 + \dots + \Big(\frac{a}{b^d}\Big)^{\ell}\bigg)\]
we now have three cases:\\\\
Case I: $\displaystyle \frac{a}{b^d}\ \lt\ 1$\\
Set $\displaystyle \tau := \frac{a}{b^d}$ then we have
\[T(n) = n^d \sum_{k=0}^{\ell} \tau^{k}\]
applying lemma 1 we get
\[T(n) = n^d\bigg(\frac{1 - \tau^{\ell + 1}}{1 - \tau}\bigg) \leq n^d\Big(\frac{1}{1-\tau}\Big)\]
but $\ds \frac{1}{1-\tau}$ is a constant so $T(n) \leq n^d$ and thus
\[T(n) = O(n^d)\]
Case II: $\ds \frac{a}{b^d} = 1$\\
Then we have that
\[T(n) = n^d \big(1 + 1 + 1^2 + \dots + 1^{\ell}\big) = n^d\big(\ell + 1\big)\]
but $n = b^{\ell} \imply \ell = \log_{b}n$, thus
\[T(n) = O(n^d\x \log n)\]
Case III: $\displaystyle \frac{a}{b^d}\ \gt\ 1$\\
Set $\displaystyle \tau := \frac{a}{b^d}$ then we have
\[T(n) = n^d \sum_{k=0}^{\ell} \tau^{k}\]
applying lemma 1 we get
\[T(n) = n^d\bigg(\frac{\tau^{\ell + 1}-1}{\tau - 1}\bigg) \leq n^d\bigg(\frac{\tau^{\ell+1}}{\tau-1}\bigg)\]
but since $\tau -1$ is a constant we get
\begin{align*}
T(n) &= n^d O(\tau^{\ell + 1})\\
	&= O(n^d\tau^{\ell + 1})\\
	&= O(n^d\tau^{\ell})\\
	&= O\bigg(\Big(\frac{a}{b^{d}}\Big)^{\ell}n^{d}\bigg)\\
	&= O\bigg(\Big(\frac{n}{b^{\ell}}\Big)^da^{\ell}\bigg)
\end{align*}
but $n = b^{\ell}$ so
\[T(n) = O(a^{\ell})\]
and $\ell = \log_ba$ so
\[T(n) = O(a^{\log_bn})\]
and applying lemma 2 gives
\[T(n) = O(n^{\log_ba})\]
This completes the proof.
\qed
\subsection{MergeSort}
MergeSort is an algorithm to sort $n$ numbers into non-decreasing order.\\\\
---------------------------------------------------------------------------------------------------------
MergeSort($x_1, x_2, \dots, x_n$)\\
	\hspace*{7mm} if $n=1$\\
	\hspace*{14mm} return $x_1$\\
	\hspace*{7mm} else\\
	\hspace*{14mm} return Merge(MergeSort($x_1, \dots, x_{\floor{\frac{n}{2}}}$), MergeSort($x_{\floor{\frac{n}{2}}+1}, \dots, x_n$))\\
---------------------------------------------------------------------------------------------------------\\
Where the Merge function is a linear time algorithm combining two sorted lists (by comparing the first element in each list and moving the smaller element of the two into our new list.)\\\\
\tb{MergeSort is correct!}
\begin{itemize}
	\item It calls itself on smaller instances until the division process terminates when it reaches a base case where each list has size 1
	\item MergeSort works trivially on the base cases
	\item This sort of strong induction-type proof works for just about all divide and conquer algorithms
\end{itemize}
\tb{MergeSort is efficient!}
\begin{itemize}
	\item The recurrence relation that we can construct to model the running time of MergeSort is given by
	\[T(n) = 2\x T\Big(\frac{n}{2}\Big) + O(n)\]
	as we break the problem into two sub-problems of size $\ds \frac{n}{2}$, plus a linear time sub-routine to combine the solutions (Merge.)
	\item This recurrence relation is in the proper form to use the Master Theorem and indeed we're in its Case II.
	\item By the master theorem the running time of MergeSort is $O(n\x \log n)$.
	\item The running time can also be proved by unwinding the recurrence.
\end{itemize}
\subsection{Binary Search}
BInary Search is an algorithm to search for whether a key $k$ exists in a given sorted list.\\\\
---------------------------------------------------------------------------------------------------------
BinarySearch($a_1, a_2, a_n ; k$)\\
	\hspace*{7mm} while $n\ \gt\ 0$\\
	\hspace*{14mm} if $\ds a_{\floor{\frac{n}{2}}} = k$\\
	\hspace*{21mm} return true\\
	\hspace*{14mm} if $\ds a_{\floor{\frac{n}{2}}}\ \gt\ k$\\
	\hspace*{21mm} return BinarySearch($a_1, \dots, a_{\ceil{\frac{n}{2}}-1} ; k$)\\
	\hspace*{14mm} if $\ds a_{\floor{\frac{n}{2}}}\ \lt\ k$\\
	\hspace*{21mm} return BinarySearch($a_{\ceil{\frac{n}{2}}+1}, \dots, a_n ; k$)\\
	\hspace*{7mm} return false\\
---------------------------------------------------------------------------------------------------------\\
\tb{Binary Search works!}
\begin{itemize}
	\item It works for the same strong induction argument as MergeSort
\end{itemize}
\tb{Binary Search is efficient!}
\begin{itemize}
	\item The recurrence relation that we can construct to model the running time of Binary Search is given by
	\[T(n) = T\Big(\frac{n}{2}\Big) + O(1)\]
	as we break the problem into one sub-problem of size $\ds \frac{n}{2}$.
	\item This recurrence relation is case II of the master theorem.
	\item By the master theorem the running time of MergeSort is $O(\log n)$.
	\item Again, the running time can also be proved by unwinding the recurrence.
\end{itemize}
\subsection{Fast Multiplication}
In COMP 250 we found an $O(n^2)$ algorithm to multiply 2 $n$-digit numbers (in any base). Let's try to do better using divide and conquer. Consider the example:\\\\
Let \tb{x} := $x_nx_{n-1}\dots x_{\frac{n}{2}+1}x_{\frac{n}{2}}\dots x_2x_1$\\
Let \tb{y} := $y_ny_{n-1}\dots y_{\frac{n}{2}+1}y_{\frac{n}{2}}\dots y_2y_1$\\\\
Then \tb{x} = $10^{\frac{n}{2}}$\tb{x$_{L}$} + \tb{x$_R$}, where \tb{x$_L$} = $x_n\dots x_{\frac{n}{2}+1}$, and \tb{x$_R$} = $x_{\frac{n}{2}}\dots x_1$\\
Similarily \tb{y} = $10^{\frac{n}{2}}$\tb{y$_{L}$} + \tb{y$_R$}, where \tb{y$_L$} = $y_n\dots y_{\frac{n}{2}+1}$, and \tb{y$_R$} = $y_{\frac{n}{2}}\dots y_1$\\\\
Thus
\begin{align*}
	\text{\tb{x}}\x \text{\tb{y}} &= (10^{\frac{n}{2}}\text{\tb{x}}_L + \text{\tb{x}}_R)(10^{\frac{n}{2}}\text{\tb{y}}_L + \text{\tb{y}}_R)\\
		&= 10^n\x \text{\tb{x}}_L \text{\tb{y}}_L + 10^{\frac{n}{2}}(\text{\tb{x}}_L\text{\tb{y}}_R + \text{\tb{x}}_R\text{\tb{y}}_L) + \text{\tb{x}}_R\text{\tb{y}}_R
\end{align*}
So we've found a divide and conquer algorithm to multiply two $n$-digit numbers. Here we're breaking the problem down into four sub-problems of size $\ds \frac{n}{2}$, with a few additions thrown in there (can be done in linear time). Therefore, the recurrence for this algorithm is given by
\[T(n) = 4\x T\Big(\frac{n}{2}\Big) + O(n)\]
then, by the master theorem, the running time of this algorithm is $O(n^2)$. Hmmm. This isn't better. Well thanks to our good friend Carl ``G-Money'' Gauss, all is not lost. Gauss was studying the product of complex numbers
\[(a + bi)(c + di) = ac - bd + (bc+ad)i\]
which seemingly involved 4 multiplications, when he noticed that actually we can do it with only 3. Note that
\[(bc+ad) = (a+b)(c+d)\]
Now we can use this exact same trick but we'll replace the $i$ with $10^{\frac{n}{2}}$. This gives
\[(\text{\tb{x}}_L\text{\tb{y}}_R + \text{\tb{x}}_R\text{\tb{y}}_L) = (\text{\tb{x}}_R + \text{\tb{x}}_L)(\text{\tb{y}}_R + \text{\tb{y}}_L) - \tb{x}_R\tb{y}_R - \tb{x}_L\tb{y}_L\]
So now we can break our problem into only 3 sub-problems! Our new recurrence is
\[T(n) = 3\x T\Big(\frac{n}{2}\Big) + O(n)\]
so finally, by the master theorem, $T(n) = O(n^{\log_2 3}) \approx O(n^{1.59})$
\subsection{Fast Matrix Multiplication}
Let's try to efficiently multiply two $n \times n$ matrices. Let
\[X :=
\begin{pmatrix}
A & B\\
C & D
\end{pmatrix}
\text{, and } Y :=
\begin{pmatrix}
E & F\\
G & H
\end{pmatrix}
\]
where $X$, and $Y$ are two $n \times n$ matrices and $A, B, \dots H$ are eight $\ds \frac{n}{2} \times \frac{n}{2}$ matrices. Then their product $Z$ is given by
\[
Z =
\begin{pmatrix}
AE + BG & AF + BH\\
CE + DG & CF + DH
\end{pmatrix}
\]
Therefore multiplying two $n \times n$ matrices involves eight products of $\ds \frac{n}{2} \times \frac{n}{2}$ matrices, and $n^2$ additions. Therefore the recurrence relation is given by
\[T(n) = 8\x T\Big(\frac{n}{2}\Big) + O(n^2)\]
By the master theorem, this then takes $O(n^3)$ time. That's ok but again, we can do better. To do better however, we need a little algebraic trick. To do this, we need to define 7 matrices, $S_1, \dots, S_7$ given by
\begin{center}
\begin{tabular}{c c}
$S_1 = (B-D)\x(G+H)$ & $S_2 = (A+D)\x(E+H)$\\
$S_3 = (A-C)\x(E+F)$ & $S_4 = (A+B)\x H$\\
$S_5 = A\x (F-H)$ & $S_6 = D\x (G-E)$\\
$S_7 = (C+D)\x E$
\end{tabular}
\end{center}
then the product of $X$, and $Y$ is given by
\[Z =
\begin{pmatrix}
S_1 + S_2 - S_4 + S_6 & S_4 + S_5\\
S_6 + S_7 & S_2 - S_3 + S_5 - S_7
\end{pmatrix}
\]
This means that we only need to do 7 multiplications! Therefore our new recurrence is
\[T(n) = 7\x T\Big(\frac{n}{2}\Big) + O(n^2)\]
and thus by the master theorem $T(n) = O(n^{\log_2 7}) \approx O(n^{2.81})$
\subsection{Fast Exponentiation}
Consider the following algorithm to quickly calculate $x^n$:\\\\
---------------------------------------------------------------------------------------------------------
FastExp($x, n$)\\
	\hspace*{7mm} if $n=1$\\
	\hspace*{14mm} return $x_1$\\
	\hspace*{7mm} else if $n$ is even\\
	\hspace*{14mm} return FastExp($x, \floor{\frac{n}{2}})^2$\\
	\hspace*{7mm} else if $n$ is odd\\
	\hspace*{14mm} return $x\x$FastExp($x, \floor{\frac{n}{2}})^2$\\
---------------------------------------------------------------------------------------------------------\\
Assuming that $n$ is a power of two, we get that recurrence is given by
\[T(n) = T\Big(\frac{n}{2}\Big) + O(1)\]
and thus by the master theorem this algorithm has time complexity of $O(\log n)$.
\subsection{The Selection Problem}
Suppose we're given a list of number and we want to find the $k^{th}$ smallest number in that list. We can use the following divide and conquer algorithm:\\\\
---------------------------------------------------------------------------------------------------------
Select($\mathcal{S}, k$)\\
	\hspace*{7mm} if $|\mathcal{S}|$ = 1\\
	\hspace*{14mm} return $x_1$\\
	\hspace*{7mm} else\\
	\hspace*{14mm} set $\mathcal{S}_L := \{x_i \in \mathcal{S} : x_i\ \lt\ x_1\}$\\
	\hspace*{14mm} set $\mathcal{S}_R := \{x_i \in \mathcal{S} : x_i \geq x_1\}$\\\\
	\hspace*{14mm} if $|\mathcal{S}_L| = k-1$\\
	\hspace*{21mm} return $x_1$\\
	\hspace*{14mm} else if $|\mathcal{S}_L|\ \gt\ k-1$\\
	\hspace*{21mm} return Select($\mathcal{S}_L , k$)\\
	\hspace*{14mm} else if $|\mathcal{S}_L|\ \lt\  k-1$\\
	\hspace*{21mm} return Select($\mathcal{S}_R , k-1-|\mathcal{S}_L|$)\\
---------------------------------------------------------------------------------------------------------\\
Well.. this actually sucks because it runs in $\Omega(n^2)$ time. Note that it would actually be faster to just sort the list and then pull the $k^{th}$ element (this can of course be done in $O(n\x \log n)$ time.) One idea that we can exploit to improve this is to use a random pivot rather than deterministically selecting $x_1$ as a pivot.\\\\
We will define a \ti{good pivot} to be such neither $\mathcal{S}_L$, nor $\mathcal{S}_R$ contain more than $\ds \frac{3}{4}$ of $\mathcal{S}$, and a \ti{bad pivot} to be otherwise. Note that by this definition the probability that a pivot will be either good or bad (respectively) is 0.5.\\\\
For randomized algorithms we're always interested in the \ti{expected runtime} of the algorithm rather than the worst case runtime. The recurrence relation, $\bar{T}(n)$, for the expected runtime for the randomized version of the selection is then given by
\[\bar{T}(n) \leq \frac{1}{2}\x \bar{T}\Big(\frac{3n}{4}\Big) + \frac{1}{2}\x \bar{T}(n) + O(n)\]
where the $\ds \frac{1}{2}\x \bar{T}\Big(\frac{3n}{4}\Big)$ term represents the recursive call given a good pivot, and the $\ds \frac{1}{2}\x \bar{T}(n)$ term represents the recursive call on a bad pivot. It's less than or equal to because we will only ever use one of those terms at once but not both. We can clean this recurrence up a bit to find the expected runtime.
\begin{align*}
	\bar{T}(n) &\leq \frac{1}{2}\x \bar{T}\Big(\frac{3n}{4}\Big) + \frac{1}{2}\x \bar{T}(n) + O(n)\\
	\frac{1}{2}\x \bar{T}(n) &\leq \frac{1}{2}\x \bar{T}\Big(\frac{3n}{4}\Big) + O(n)\\
	\bar{T}(n) &\leq \bar{T}\Big(\frac{3n}{4}\Big) + O(n)
\end{align*}
Applying the master theorem to that recurrence gives $\bar{T}(n) = O(n)$\\\\
This is good but let's try to do better, i.e. let's see if we can find a deterministic linear time algorithm. It turns out we can but it requires a little trick called the \ti{median of medians}.\\\\
It's best to illustrate this concept with an example. Suppose we have a list, $\mathcal{S} = \{x_1, x_2, \dots, x_n\}$, from which we want to find the median of medians. To do this we partition the list into $\ds \Bigceil{\frac{n}{5}}$ groups of cardinality 5. Denote each such group by $G_1, \dots, G_{\ceil{\frac{n}{5}}}$ Then we'll sort each group and let $z_i$ be the median of each group $G_i$. Let $m$ be the median of the set $\mathcal{Z} = \{z_1, \dots, z_{\ceil{\frac{n}{5}}}\}$. Predictably, we call $m$ the \ti{median of medians}. This is significant because $m$ will always be a good pivot. This is because there will always be at least $\ds \frac{3n}{10} - 1$ numbers in $\mathcal{S}$ less than $m$, and $\ds \frac{3n}{10} - 1$ numbers in $\mathcal{S}$ at least as large as $m$. Therefore $\ds |\mathcal{S}_R| \leq \frac{7n}{10}$, and $\ds |\mathcal{S}_L| \leq \frac{7n}{10}$.\\\\
Now that we have a deterministic way to guarntee we have a good pivot, all that's left is to bake it into a selection algorithm. Luckily, our algorithm won't vary too much since all we have to do is add a few steps at the beginning.\\\\\\
---------------------------------------------------------------------------------------------------------
DetSelect($\mathcal{S}, k$)\\
	\hspace*{7mm} if $|\mathcal{S}| = 1$\\
	\hspace*{14mm} return $x_1$\\
	\hspace*{7mm} else\\
	\hspace*{14mm} partition $\mathcal{S}$ into $\ceil{\frac{n}{5}}$ groups of 5\\
	\hspace*{14mm} for $j$ in range $\big(1, \ceil{\frac{n}{5}}\big)$\\
	\hspace*{21mm} let $z_j$ be the median of the group $G_j$.\\
	\hspace*{14mm} let $\mathcal{Z} = \{z_1, \dots, z_{\ceil{\frac{n}{5}}}\}$\\
	\hspace*{14mm} set $m$ := DetSelect\big($\mathcal{Z}, \ceil{\frac{n}{10}}$\big)\\\\
	\hspace*{14mm} set $\mathcal{S}_L := \{x_i \in \mathcal{S} : x_i\ \lt\ m\}$\\
	\hspace*{14mm} set $\mathcal{S}_R := \{x_i \in \mathcal{S} : x_i \geq m\}$\\\\
	\hspace*{14mm} if $|\mathcal{S}_L| = k-1$\\
	\hspace*{21mm} return $m$\\
	\hspace*{14mm} if $|\mathcal{S}_L|\ \gt\ k-1$\\
	\hspace*{21mm} return DetSelect($\mathcal{S}_L, k$)\\
	\hspace*{14mm} if $|\mathcal{S}_L|\ \lt\ k-1$\\
	\hspace*{21mm} return DetSelect($\mathcal{S}_R, k-1-|\mathcal{S}_L|$)\\
---------------------------------------------------------------------------------------------------------\\
The recursive formula for the running time of this algorithm is
\[T(n) \leq T\bigg(\frac{7n}{10}\bigg) + T\bigg(\frac{n}{5}\bigg) + O(n)\]
where the $\ds T\bigg(\frac{n}{5}\bigg)$ term is the time to find the median of medians, the $\ds T\bigg(\frac{7n}{10}\bigg)$ term is pivoting based on the median of medians. Unfortunately, this doesn't work with the master theorem and we can't simplify it down to a form that does (unlike with the randomized algorithm, we need both of the $T(\dots)$ terms.) Since it doesn't work with the master theorem, we need to use the recusion tree method. From the recursion tree method, $T(n) = O(n)$. So there it is.. a linear time deterministic algorithm to select the $k^{th}$ smallest element from a list!
\subsection{The Closest Pair of Points in a Plane}
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Graph Algorithms}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Greedy Algorithms}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Dynamic Programming}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Network Flows}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data Structures}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}